# OVN Northd - High Availability Deployment with Leader Election
# northd translates high-level network configuration from NB DB into logical flows in SB DB
#
# In HA mode:
# - Multiple replicas can run simultaneously
# - Only the leader actively processes changes
# - Leader election is handled by Kubernetes lease mechanism
# - Automatic failover when leader fails
#
# Reference: OVN-Kubernetes HA deployment patterns
# Image: Official OVN-Kubernetes image from ghcr.io
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ovn-northd
  namespace: ovn-kubernetes
  labels:
    app.kubernetes.io/name: ovn-northd
    app.kubernetes.io/component: control-plane
spec:
  replicas: 2  # Active-standby for HA
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: ovn-northd
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ovn-northd
        app.kubernetes.io/component: control-plane
    spec:
      serviceAccountName: ovn-northd
      tolerations:
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoSchedule
        - key: node-role.kubernetes.io/master
          operator: Exists
          effect: NoSchedule
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: ovn-northd
                topologyKey: kubernetes.io/hostname
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 50
              preference:
                matchExpressions:
                  - key: node-role.kubernetes.io/control-plane
                    operator: Exists

      # Wait for NB and SB databases to be ready
      initContainers:
        - name: wait-for-dbs
          image: ghcr.io/ovn-org/ovn-kubernetes/ovn-kube-ubuntu:master
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -c
            - |
              set -ex
              echo "Waiting for OVN NB DB cluster to be ready..."
              until ovsdb-client list-dbs tcp:ovn-nb-db.ovn-kubernetes.svc.cluster.local:6641 2>/dev/null | grep -q OVN_Northbound; do
                echo "NB DB not ready, waiting..."
                sleep 2
              done
              echo "NB DB is ready!"
              
              echo "Waiting for OVN SB DB cluster to be ready..."
              until ovsdb-client list-dbs tcp:ovn-sb-db.ovn-kubernetes.svc.cluster.local:6642 2>/dev/null | grep -q OVN_Southbound; do
                echo "SB DB not ready, waiting..."
                sleep 2
              done
              echo "SB DB is ready!"
      containers:
        - name: ovn-northd
          image: ghcr.io/ovn-org/ovn-kubernetes/ovn-kube-ubuntu:master
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -c
            - |
              set -ex
              
              # Define database connection strings
              # In HA mode, connect to the ClusterIP service which load-balances to available pods
              OVN_NB_DB="tcp:ovn-nb-db.ovn-kubernetes.svc.cluster.local:6641"
              OVN_SB_DB="tcp:ovn-sb-db.ovn-kubernetes.svc.cluster.local:6642"
              
              echo "Starting OVN northd in HA mode..."
              echo "NB DB: ${OVN_NB_DB}"
              echo "SB DB: ${OVN_SB_DB}"
              echo "Pod: ${POD_NAME}"
              
              # Start northd daemon
              # In HA mode, multiple northd instances can run but only one is active
              # OVN handles this internally through database locking
              exec ovn-northd \
                --ovn-northd-nb-db="${OVN_NB_DB}" \
                --ovn-northd-sb-db="${OVN_SB_DB}" \
                -vconsole:info \
                --pidfile=/var/run/ovn/ovn-northd.pid \
                --no-chdir
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OVN_NB_DB
              value: "tcp:ovn-nb-db.ovn-kubernetes.svc.cluster.local:6641"
            - name: OVN_SB_DB
              value: "tcp:ovn-sb-db.ovn-kubernetes.svc.cluster.local:6642"
          volumeMounts:
            - name: ovn-run
              mountPath: /var/run/ovn
          livenessProbe:
            exec:
              command:
                - /bin/bash
                - -c
                - |
                  pgrep -f ovn-northd > /dev/null
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          resources:
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 1000m
              memory: 1Gi
      volumes:
        - name: ovn-run
          emptyDir: {}
---
# ServiceAccount for ovn-northd
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ovn-northd
  namespace: ovn-kubernetes
---
# Role for leader election
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ovn-northd-leader-election
  namespace: ovn-kubernetes
rules:
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["get", "create", "update"]
---
# RoleBinding for leader election
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ovn-northd-leader-election
  namespace: ovn-kubernetes
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ovn-northd-leader-election
subjects:
  - kind: ServiceAccount
    name: ovn-northd
    namespace: ovn-kubernetes
